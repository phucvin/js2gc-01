Summary: Added `scripts/run_testdata_with_js_interpreter.py` and saved output to `scratch/js_interpreter/test.out`.

# User Prompt

add scripts/run_testdata_with_js_interpreter.py to run js files in testdata with js_interpreter

mention the script in js interpreter readme

also follow agents.md

# Agent Response

I created `scripts/run_testdata_with_js_interpreter.py` which iterates over `.js` files in `testdata`, executes them using `scratch/js_interpreter/js_interpreter`, and compares the output with `.out` files.
I built the `js_interpreter` binary to verify the script.
I updated `scratch/js_interpreter/README.md` to include instructions for running the test data using the new script.
I verified the script runs (13 passed, 6 failed due to output differences which are expected/out of scope for this task).

# User Prompt

run and write test results output to scratch/js_interpreter/test.out

Carefully consider the request and update the plan with `set_plan` tool if needed.

# Agent Response

I executed `python3 scripts/run_testdata_with_js_interpreter.py > scratch/js_interpreter/test.out` to run the tests and save the output.
I verified the content of `scratch/js_interpreter/test.out`.
I am now submitting the changes (including the new output file).
